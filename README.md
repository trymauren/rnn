# This repository contains code for "RNN - an educational implementation".

Note: The original repo was deleted due to the issues mentioned [here](https://stackoverflow.com/questions/34579211/how-to-delete-a-file-tracked-by-git-lfs-and-release-the-storage-quota). This repo is a copy of the old one, but does not contain any pretrained models.

All common functions and classes resides in `utils`. The `rnn` directory contains the rnn code. Data used for training the networks can be found in `data`. Files for running experiments conducted and shown in the report, can be found in the directory `run-nlp` (natural language processing/generation experiments), and `run-sine` (sine wave prediction experiments). Unfortunately, pretrained models are not present in this repo anymore.

# Example installation of python packages required

```
virtualenv my_new_environment
. my_new_environment/bin/activate
python -m pip install -r requirements.txt
```

# Usage of the RNN

## Initialization
 - Classification (example: text processing):
    ```python
    from rnn.rnn import RNN

    rnn = RNN(
            hidden_activation='Tanh()',
            output_activation='Softmax()',
            loss_function='Classification_Logloss()',
            optimiser='AdaGrad()',
            learning_rate=0.001,
            seed=24
            )
    ```

- Regression (example: sine wave prediction):
    ```python
    rnn = RNN(
            hidden_activation='Tanh()',
            output_activation='Identity()',  
            loss_function='mse()',
            optimiser='AdaGrad()',
            learning_rate=0.001,
            seed=24
            )
    ```

## Training

### Data formatting

- Text as word embeddings:
    ```python
    import sys
    import git
    path_to_root = git.Repo('.', search_parent_directories=True).working_dir
    sys.path.append(path_to_root)
    import utils.text_processing as text_proc
    from utils.text_processing import WORD_EMBEDDING

    word_emb = WORD_EMBEDDING()
    text_data = text_proc.read_file(path_to_root + '/data/harry_potter.txt')

    X, y = np.array(word_emb.translate_and_shift(text_data))

    X = np.array([X])

    vocab, inverse_vocab = text_proc.create_vocabulary(X)
    y = text_proc.create_labels(X, inverse_vocab)

    # The data must then be reshaped. To process all substrings of length 24 in a batch:
    num_sequences, seq_length, features = 100, 24, X.shape[-1]
    output_size = y.shape[-1]

    inputs = np.zeros((1, num_sequences, seq_length, features))
    targets = np.zeros((1, num_sequences, seq_length, output_size))

    # Sliding window
    for i in range(num_sequences):
        input_ = X[0, i:i + seq_length, :]
        target = y[0, i + 1:i+seq_length+1, :]
        inputs[0, i] = input_
        targets[0, i] = target

    # Take the transpose to process all substrings generated by sliding window, in parallel.
    X = inputs.transpose(0, 2, 1, 3)
    y = targets.transpose(0, 2, 1, 3)
    ```

- Text as one-hot-encoded characters:
    ```python
    import sys
    import git
    path_to_root = git.Repo('.', search_parent_directories=True).working_dir
    sys.path.append(path_to_root)
    import utils.text_processing as text_proc

    text_data = text_proc.read_file(path_to_root + '/data/harry_potter.txt')
    chars = sorted(list(set(text_data)))  # to keep the order consistent over runs
    data_size, vocab_size = len(text_data), len(chars)

    char_to_ix = {ch: i for i, ch in enumerate(chars)}
    ix_to_char = {i: ch for i, ch in enumerate(chars)}

    num_sequences, seq_length = 100, 24

    X = np.zeros((num_sequences, seq_length, 1, vocab_size))
    y = np.zeros((num_sequences, seq_length, 1, vocab_size))

    # Sliding window
    for i in range(num_sequences):
        inputs = [char_to_ix[ch] for ch in text_data[i:i + seq_length]]
        targets = [char_to_ix[ch] for ch in text_data[i + 1:i+seq_length+1]]
        onehot_x = text_proc.create_onehot(inputs, char_to_ix)
        onehot_y = text_proc.create_onehot(targets, char_to_ix)
        X[i] = onehot_x
        y[i] = onehot_y

    X = X.transpose((2, 1, 0, 3))
    y = y.transpose((2, 1, 0, 3))
    ```

- Numeric / sine data:
    ```python

    def create_sines(examples=10, seq_length=100):
    X = []
    y = []
    for _ in range(examples):
        example_x = np.array(
            [np.sin(
                np.linspace(0, 8*np.pi, seq_length+1))]
            ).T
        X.append(example_x[0:-1])
        y.append(example_x[1:])

    return np.array(X), np.array(y)

    X, y = create_sines(examples=examples, seq_length=seq_length)

    X = X.reshape(examples, -1, num_batches, 1)
    y = y.reshape(examples, -1, num_batches, 1)
    ```

### Running training

- Text data:
    ```python
    hidden_state = rnn.fit(
        X,
        y,
        epo=1000,
        num_hidden_nodes=300, # How many iterations over all the training data to run.
        unrolling_steps=24, # May be adjusted, controls number of forward/backward passes at a time.
        vocab=ix_to_char # Use vocab instead when doing embedding experiments (see embedding example above)
        )
    ```

- Numeric / sine data:
    ```python
    hidden_state = rnn.fit(
        X,
        y,
        epo=1000, # How many iterations over all the training data to run.
        num_hidden_nodes=100, # Adjusts the hidden size. Approx. 100 is good if training on sine waves.
        )
    ```

### Early stopping
In both training cases seen above, the keyword-parameters `X_val` and `y_val` accepts datasets that are used to validate the model (store validation loss). Both these datasets should be initialized the same way as the regular training datasets, as shown in the previous section. By passing an integer to `num_epochs_no_update` keyword-parameter, training will stop if validation loss has increased for more than `num_epochs_no_update`. 
The loss can be plotted by simply calling plot_loss() on the model after training:

```python
rnn.plot_loss(plt, show=True, val=True/False)
```

## Prediction

 - Text data:

    To predict, a seed/primer must be passed to the model. This is for the model to adjust its hidden state from `0`. In addition, a keyword-parameter `time_steps_to_generate` specifies how many timesteps (values) which should be generated. Since the return values is word-embeddings or one-hot vectors, they must be converted back to text. For embeddings, this is achieved by using `find_closest()` method from the `WORD_EMBEDDING` class imported in the example above. For one-hot-encoded characters, this is achieved by using the one_hot_to_ix method from the `text_processing` util.
    
    ```python
    # Word-embedding case
    from utils.text_processing import WORD_EMBEDDING
    word_emb = WORD_EMBEDDING()
    predict = rnn.predict(X_seed, time_steps_to_generate=10)
    for emb in predict:
        print(word_emb.find_closest(emb, 1))
    ```

    ```python
    # Character case
    from utils.text_processing import onehot_to_ix
    predict = rnn.predict(X_seed, time_steps_to_generate=10)
    for char in predict:
        ix = onehot_to_ix(char)
        pred += ix_to_char[ix] # the creation of ix_to_char is shown above
    ```

 - Numeric / sine data:
    ```python
    predict = rnn.predict(X_seed, time_steps_to_generate=10)
    ```

    The predicted data can be plotted as follows to show it:


## Loading models

If the model is saved by passing the `name` parameter at initialisation, it can be loaded from disk by using the load_model() function.
```python
rnn = RNN(
    hidden_activation='Tanh()',
    output_activation='Softmax()',
    loss_function='Classification_Logloss()',
    optimiser='AdaGrad()',
    clip_threshold=np.inf,
    name='/directory/of/choice/new_trained_model', # Path to save directory is given here
    learning_rate=0.001,
    )
rnn.fit(X, y, epo=100)

from utils.read_load_model import load_model
rnn = load_model('/directory/of/choice/new_trained_model')
```
